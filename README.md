# trial_task
First, the necessary libraries should be installed:

pip install -U spacy --no-cache-dir
python -m spacy download en_core_web_lg
pip install -U sentence-transformers

We assume that the user will input the text through a text file. The path to the file is entered in CLI. For demonstration I placed a file named text.txt, containing a sample business text, in the working directory. In the same directory there is a text file with a list of commonly used business English phrases. Both the sample text and the phrases were generated by gpt-4.

The process basically comprises 4 stages: 
 1. Extract phrases from the given text 
 2. Prepare pre-loaded phrases 
 3. Obtain embeddings for the pre-loaded phrases and the phrases from the text
 4. Calculate cosine similarity scores between the 2 sets of embeddings and get the pairs of phrases with the highest scores

 1. We first read the file with the text and remove all extra space characters. We break the text into sentences using nltk library, and then process the sentences with spaCy - a widely-known NLP library. It performs tokenisation, lemmatisation, as well as POS tagging and syntactic parsing. We then find verbs and apply the function that returns the objects of a given verb. The function deals with both direct and indirect objects (with a preposition). It not only extracts the object but also some of its left and right children.
We also use another function to extract noun phrases from the sentence, i.e. a noun with some of its left and right children. We collect verb and noun phrases in separate lists.
 2. The list of prepared business English phrases is then loaded from a file and also processed with spacy. This is done to identify parts of speech and thus distinguish between verb and noun phrases. The phrases are split into 2 groups and stored in different lists.
 3. We try two ways of getting sentence embeddings. The first way is to make use of sentence-transformers library, which allows us to get one embedding for the whole sentence without having to average word embeddings. The model we use here is distil-roberta. We encode separately the lists of noun and verb phrases, so that we can match verb and noun phrases from the text against the standardised verb and noun phrases.
Another way is to obtain word embeddings from roberta-base model and average them on every coordinate (mean pooling).
4. We calculate cosine similarity scores between all the pairs of input phrases and standard phrases. Then for each of the input phrases the maximum score is found and the standard phrase corresponding to that score is picked. If the cosine score is greater than 0.5, we assume that this phrase can be considered as a possible replacement for the given phrase. Thus, we output the list of phrases for the input text with a possible replacement for each phrase and the corresponding cosine score; if there is no proper replacement (all the cosine scores for that phrase were less than 0.5), the phrase is ommited, since there are no options to replace it.
