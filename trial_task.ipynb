{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiNgyx4yWV/Z950bElBu3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JennyFrost/trial_task/blob/main/trial_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy --no-cache-dir\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Trs2RYEjYzU",
        "outputId": "2a67b8c7-7d5a-4153-e531-213f8ec0d82c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Installing collected packages: cloudpathlib, weasel, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpathlib-0.16.0 spacy-3.7.2 weasel-0.3.4\n",
            "2024-01-20 23:03:37.603881: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-20 23:03:37.603953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-20 23:03:37.606183: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-20 23:03:37.619849: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-20 23:03:39.796242: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=13ca51b375e11b574849ffa55cd2ca08e738d5f568e08123635170ae372bea8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from itertools import chain\n",
        "import torch\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, RobertaModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "SEt6tdwRjYxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf30c14-3be0-4ac7-bc1d-f7d298ec3678"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the input text and the standardised phrases ##"
      ],
      "metadata": {
        "id": "yaXefXPmNQvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = input('Enter the path to the file with the text you wish to improve: ')\n",
        "\n",
        "# place the text file in the folder 'content'\n",
        "if filename:\n",
        "    with open(file=filename, mode='r') as f:\n",
        "        lines = f.readlines()\n",
        "    input_text = ' '.join([line.strip() for line in lines if not re.match(r'\\s+', line)])\n",
        "    input_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG180GNMVaQW",
        "outputId": "c04fa673-bba2-4701-8f39-550a340a5518"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path to the file with the text you wish to improve: /content/text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# place the file with standard phrases in the folder 'content'\n",
        "filename_phrases = '/content/business phrases.txt'\n",
        "with open(file=filename_phrases, mode='r') as f:\n",
        "    lines = f.readlines()\n",
        "standard_phrases = list(set([phrase.strip().lower() for phrase in lines]))\n",
        "len(standard_phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8bRSDxoX1vh",
        "outputId": "9411f028-661b-4403-b4d0-6117b4d54f91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "526"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting verb and noun phrases from text ##"
      ],
      "metadata": {
        "id": "21YK2unXNC0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sentence_decomposition(sentence_doc, print_sentence=True, print_lefts_and_rights=True):\n",
        "    if print_sentence:\n",
        "        print(sentence_doc, \"\\n\")\n",
        "\n",
        "    if print_lefts_and_rights:\n",
        "        for token in sentence_doc:\n",
        "            print(\"{} {:25s} {:10s} {:10s} {:20s} {:20s} {}\".format(token.i, token.text, token.pos_, token.dep_,\n",
        "                                                                    token.head.text, \"['\" + \"','\".join(\n",
        "                    [x.text for x in token.lefts]) + \"']\", \"['\" + \"','\".join([x.text for x in token.rights]) + \"']\"))\n",
        "\n",
        "    else:\n",
        "        for token in sentence_doc:\n",
        "            print(\"{} {:25s} {:10s} {:10s} {:20s}\".format(token.i, token.text, token.pos_, token.dep_, token.head.text))"
      ],
      "metadata": {
        "id": "W01bWvNvNCCo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PhraseExtractor:\n",
        "\n",
        "    def __init__(self, docs: list[spacy.tokens.doc.Doc]):\n",
        "        self.docs = docs\n",
        "\n",
        "    @staticmethod\n",
        "    def get_action_verbs(doc: spacy.tokens.doc.Doc) -> list[spacy.tokens.token.Token]:\n",
        "        \"\"\"\n",
        "            For a sentence processed with spaCy, finds all the verbs\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "            doc : Doc\n",
        "                sentence processed with spaCy\n",
        "            Output\n",
        "            ------\n",
        "            verbs: list[Token]\n",
        "                list of the verb tokens found (in spaCy format)\n",
        "\n",
        "        \"\"\"\n",
        "        verbs = []\n",
        "        for token in doc:\n",
        "            if token.pos_ in ('VERB', 'AUX'):\n",
        "                if token.dep_ == 'amod' and token.head.dep_ != 'ROOT':\n",
        "                    continue\n",
        "                if token.i >= 2:\n",
        "                    if (re.search(r'ed\\b', token.text)\n",
        "                                            and (((doc[token.i-1].text == ','\n",
        "                                            and doc[token.i-2].pos_ == 'ADJ')\n",
        "                                            or doc[token.i-1].pos_ == 'ADJ')\n",
        "                                            or (token.dep_ in ('conj','appos')\n",
        "                                            and not re.search(r'ed\\b', token.head.text)))):\n",
        "                        continue\n",
        "                verbs.append(token)\n",
        "            elif token.pos_ == 'ADJ' and bool(re.search(r'ed\\b|ing\\b', token.text)) \\\n",
        "                    and token.head.dep_ in ('ROOT', 'nsubj') \\\n",
        "                    and not ((doc[token.i - 1].text == ',' and\n",
        "                              doc[token.i - 2].pos_ == 'ADJ') or\n",
        "                              doc[token.i - 1].pos_ == 'ADJ'):\n",
        "                verbs.append(token)\n",
        "        return verbs\n",
        "\n",
        "    @staticmethod\n",
        "    def get_verb_objects(doc: spacy.tokens.doc.Doc,\n",
        "                         verb: spacy.tokens.token.Token) -> list[str]:\n",
        "        \"\"\"\n",
        "            For sentence processed with spacy and verb processed with spacy finds direct objects\n",
        "            and prepositional phrases of the verb\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "            doc : Doc\n",
        "                sentence processed with spacy\n",
        "            verb: Doc\n",
        "                verb processed with spacy\n",
        "            Output\n",
        "            ------\n",
        "            obj_text/pobj_text: list\n",
        "                a list of strings (objects of the verb) with left children\n",
        "        \"\"\"\n",
        "        if 'dobj' in list(map(lambda x: x.dep_, verb.rights)):\n",
        "            obj = [tok for tok in verb.rights if tok.dep_ == 'dobj'][0]\n",
        "            rights = list(chain([x for x in verb.rights if x.i > obj.i], list(obj.rights)))\n",
        "            objs = [obj]\n",
        "            obj_text = [' '.join([x.text for x in verb.rights if x.i < obj.i])]\n",
        "            if obj.conjuncts:\n",
        "                conjs = [doc[conj.i:conj.i+1] for conj in obj.conjuncts]\n",
        "                rights += list(chain.from_iterable([list(conj[-1].rights) for conj in conjs]))\n",
        "                objs += conjs\n",
        "                obj_text += [conj.text for conj in conjs]\n",
        "            for i, obj in enumerate(objs):\n",
        "                lefts = [tok.text for tok in obj.subtree if tok.i <= obj.i]\n",
        "                if lefts:\n",
        "                    obj_text[i] += ' '.join(lefts)\n",
        "            if 'ADP' in list(map(lambda x: x.pos_, rights)):\n",
        "                prep = list(filter(lambda x: x.pos_ == 'ADP', rights))[0]\n",
        "                for i, obj in enumerate(objs):\n",
        "                    obj_text[i] += ' ' + prep.text\n",
        "                if [tok for tok in prep.rights if tok.dep_ == 'pobj']:\n",
        "                    pobj = [tok for tok in prep.rights if tok.dep_ == 'pobj'][0]\n",
        "                    pobj_lefts = [tok.text for tok in pobj.subtree if tok.i <= pobj.i or tok.pos_ == 'ADP']\n",
        "                    if len(pobj_lefts) > 1:\n",
        "                        for i, obj in enumerate(objs):\n",
        "                            obj_text[i] += ' ' + ' '.join(pobj_lefts)\n",
        "            return obj_text\n",
        "        if any(list(map(lambda word: word.pos_ == 'ADP', verb.rights))):\n",
        "            prep = [word for word in verb.rights if word.pos_ == 'ADP'][0]\n",
        "            if [word for word in prep.rights if word.dep_ == 'pobj']:\n",
        "                pobj = [word for word in prep.rights if word.dep_ == 'pobj'][0]\n",
        "                pobjs = [pobj]\n",
        "                pobj_text = [' '.join([x.text for x in verb.rights if x.i < pobj.i])]\n",
        "                if pobj.conjuncts:\n",
        "                    pobjs += list(pobj.conjuncts)\n",
        "                    pobj_text.append(' '.join([conj.text for conj in pobj.conjuncts]))\n",
        "                for i, obj in enumerate(pobjs):\n",
        "                    lefts = [tok.text for tok in pobj.subtree if tok.i <= pobj.i]\n",
        "                    if lefts:\n",
        "                        pobj_text[i] += ' ' + ' '.join(lefts)\n",
        "                    if 'ADP' in list(map(lambda x: x.pos_, pobj.rights)):\n",
        "                        prep2 = list(filter(lambda x: x.pos_ == 'ADP', pobj.rights))[0]\n",
        "                        if [tok for tok in prep2.rights if tok.dep_ == 'pobj']:\n",
        "                            pobj2 = [tok for tok in prep2.rights if tok.dep_ == 'pobj'][0]\n",
        "                            pobj_lefts2 = [tok.text for tok in pobj2.subtree if tok.i <= pobj2.i or tok.pos_ == 'ADP']\n",
        "                            if len(pobj_lefts2) > 1:\n",
        "                                for i, pobj in enumerate(pobjs):\n",
        "                                    pobj_text[i] += ' ' + prep2.text + ' ' + ' '.join(pobj_lefts2)\n",
        "                return pobj_text\n",
        "\n",
        "    @staticmethod\n",
        "    def get_noun_phrases(doc: spacy.tokens.doc.Doc) -> list[str]:\n",
        "        \"\"\"\n",
        "            For sentence processed with spacy finds noun phrases (noun and its children)\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "            doc : Doc\n",
        "                sentence processed with spacy\n",
        "            Output\n",
        "            ------\n",
        "            noun_phrases: list[str]\n",
        "                a list of strings of noun phrases\n",
        "        \"\"\"\n",
        "        noun_phrases = []\n",
        "        for token in doc:\n",
        "            if token.pos_ in ('NOUN', 'PROPN'):\n",
        "                conj = [tok for tok in token.subtree if tok.dep_ == 'conj']\n",
        "                conj_ind = len(doc)\n",
        "                if conj:\n",
        "                    conj_ind = conj[0].i\n",
        "                    if doc[conj_ind-1].pos_ in ('CCONJ', 'PUNCT'):\n",
        "                        conj_ind -= 1\n",
        "                else:\n",
        "                    if [tok for tok in token.subtree if tok.pos_ == 'PUNCT']:\n",
        "                        conj_ind = [tok for tok in token.subtree if tok.pos_ == 'PUNCT'][0].i\n",
        "                noun_phrase = [tok for tok in token.subtree if tok.pos_ != 'PRON' and tok.i < conj_ind]\n",
        "                if noun_phrase:\n",
        "                    if noun_phrase[0].pos_ == 'DET':\n",
        "                        noun_phrase = noun_phrase[1:]\n",
        "                    if len(noun_phrase) > 1:\n",
        "                        noun_phrases.append(' '.join(list(map(lambda x: x.text, noun_phrase))))\n",
        "        return noun_phrases\n",
        "\n",
        "    def get_phrases_from_text(self) -> tuple[list[str], list[str]]:\n",
        "        \"\"\"\n",
        "            For a list of sentences processed with spacy, makes verb phrases of verb + its object\n",
        "            and collects all the verb phrases and noun phrases\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "            doc : Doc\n",
        "                sentence processed with spacy\n",
        "            Output\n",
        "            ------\n",
        "            noun_phrases: list[str]\n",
        "                a list of strings of noun phrases\n",
        "            verb_phrases: list[str]\n",
        "                a list of strings of verb phrases\n",
        "        \"\"\"\n",
        "        verb_phrases_from_text = []\n",
        "        noun_phrases_from_text = []\n",
        "        for doc in self.docs:\n",
        "            verbs = self.get_action_verbs(doc)\n",
        "            print(doc)\n",
        "            if verbs:\n",
        "                for verb in verbs:\n",
        "                    if self.get_verb_objects(doc, verb):\n",
        "                        phrase = verb.lemma_ + ' ' + self.get_verb_objects(doc, verb)[0]\n",
        "                        verb_phrases_from_text.append(phrase)\n",
        "                        print(phrase)\n",
        "            noun_phrases = self.get_noun_phrases(doc)\n",
        "            if noun_phrases:\n",
        "                noun_phrases_from_text.extend(noun_phrases)\n",
        "                for phrase in noun_phrases:\n",
        "                    print(phrase)\n",
        "            print('=============================================================')\n",
        "        return verb_phrases_from_text, noun_phrases_from_text"
      ],
      "metadata": {
        "id": "vGaVn82jXtws"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_docs = list(nlp.pipe(sent_tokenize(input_text)))\n",
        "input_docs"
      ],
      "metadata": {
        "id": "l6C3zsodYJLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ffab22-681c-4f30-ff96-340102497b3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[I am thrilled to share some exciting news with you!,\n",
              " Our recent sales figures have shown a significant increase, and it's all thanks to the hard work and dedication of our team.,\n",
              " This is a fantastic achievement, and I wanted to take a moment to express my gratitude for your substantial contribution to this success.,\n",
              " Your efforts in driving sales and engaging with our customers have been instrumental in reaching our targets.,\n",
              " Additionally, I wanted to touch base regarding our ongoing marketing campaign.,\n",
              " The initial feedback and results have been quite promising.,\n",
              " The innovative approach and creative strategies employed by the marketing team are resonating well with our target audience, which in turn is positively impacting our sales.,\n",
              " As we continue to ride this wave of success, I believe it's crucial to maintain our momentum.,\n",
              " This would be a good time to review our sales tactics and align them even more closely with the marketing strategies to maximize our impact in the market.,\n",
              " I'm confident that by continuing to work collaboratively with the marketing team, we can amplify our reach and further boost our sales figures.,\n",
              " I would like to schedule a meeting next week to discuss how we can further capitalize on our current success and explore new sales strategies.,\n",
              " Please let me know your availability so we can set a time that works for you.]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "standard_phrases_docs = list(nlp.pipe(standard_phrases))\n",
        "standard_phrases_docs[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG5PdpiwIUjs",
        "outputId": "4d384ad4-68f5-46c0-fb69-17e2a8d96acf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[market potential,\n",
              " manage workflows,\n",
              " increase revenue,\n",
              " employee engagement,\n",
              " capital investment,\n",
              " globalization strategy,\n",
              " sales forecasting,\n",
              " lean management,\n",
              " business success,\n",
              " brand strategy,\n",
              " enterprise resource planning,\n",
              " decision making,\n",
              " strengthen capabilities,\n",
              " profit margins,\n",
              " execute plans,\n",
              " maximize output,\n",
              " negotiate deals,\n",
              " strategic objectives,\n",
              " operational planning,\n",
              " maximize potential]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verb_standard_phrases, noun_standard_phrases = [], []\n",
        "for doc in standard_phrases_docs:\n",
        "    root = [tok for tok in doc if tok.dep_ == 'ROOT'][0]\n",
        "    if root.pos_ in ('VERB', 'AUX'):\n",
        "        verb_standard_phrases.append(doc.text)\n",
        "    else:\n",
        "        noun_standard_phrases.append(doc.text)\n",
        "\n",
        "print('Examples of verb phrases: \\n', *verb_standard_phrases[:10], sep='\\n', end='\\n\\n')\n",
        "print('Examples of noun phrases: \\n', *noun_standard_phrases[:10], sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKKnyn7TIfa9",
        "outputId": "13a3e36b-30bc-4447-e73e-1dc20a429bfa"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples of verb phrases: \n",
            "\n",
            "manage workflows\n",
            "increase revenue\n",
            "strengthen capabilities\n",
            "execute plans\n",
            "maximize output\n",
            "negotiate deals\n",
            "maximize potential\n",
            "reduce waste\n",
            "develop techniques\n",
            "protect assets\n",
            "\n",
            "Examples of noun phrases: \n",
            "\n",
            "market potential\n",
            "employee engagement\n",
            "capital investment\n",
            "globalization strategy\n",
            "sales forecasting\n",
            "lean management\n",
            "business success\n",
            "brand strategy\n",
            "enterprise resource planning\n",
            "decision making\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_extractor = PhraseExtractor(input_docs)\n",
        "verb_phrases_from_text, noun_phrases_from_text = phrase_extractor.get_phrases_from_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy6N8WbpRYlx",
        "outputId": "a411676d-23e3-40e9-a1c8-0734391de31d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am thrilled to share some exciting news with you!\n",
            "share some exciting news with\n",
            "exciting news\n",
            "=============================================================\n",
            "Our recent sales figures have shown a significant increase, and it's all thanks to the hard work and dedication of our team.\n",
            "show a significant increase\n",
            "recent sales figures\n",
            "significant increase\n",
            "thanks to the hard work\n",
            "hard work\n",
            "=============================================================\n",
            "This is a fantastic achievement, and I wanted to take a moment to express my gratitude for your substantial contribution to this success.\n",
            "take a moment\n",
            "express my gratitude for your substantial contribution to\n",
            "fantastic achievement\n",
            "gratitude for substantial contribution to this success\n",
            "substantial contribution to this success\n",
            "=============================================================\n",
            "Your efforts in driving sales and engaging with our customers have been instrumental in reaching our targets.\n",
            "drive sales\n",
            "engage with our customers\n",
            "reach our targets\n",
            "efforts in driving sales\n",
            "=============================================================\n",
            "Additionally, I wanted to touch base regarding our ongoing marketing campaign.\n",
            "touch base\n",
            "ongoing marketing campaign\n",
            "=============================================================\n",
            "The initial feedback and results have been quite promising.\n",
            "initial feedback\n",
            "=============================================================\n",
            "The innovative approach and creative strategies employed by the marketing team are resonating well with our target audience, which in turn is positively impacting our sales.\n",
            "employ by the marketing team\n",
            "resonate well with our target audience\n",
            "impact our sales\n",
            "innovative approach and creative\n",
            "marketing team\n",
            "target audience\n",
            "=============================================================\n",
            "As we continue to ride this wave of success, I believe it's crucial to maintain our momentum.\n",
            "ride this wave of\n",
            "maintain our momentum\n",
            "wave of success\n",
            "=============================================================\n",
            "This would be a good time to review our sales tactics and align them even more closely with the marketing strategies to maximize our impact in the market.\n",
            "review our sales tactics\n",
            "align them with the marketing strategies in\n",
            "maximize our impact in the market\n",
            "good time to review sales tactics\n",
            "sales tactics\n",
            "marketing strategies to maximize impact in the market\n",
            "impact in the market\n",
            "=============================================================\n",
            "I'm confident that by continuing to work collaboratively with the marketing team, we can amplify our reach and further boost our sales figures.\n",
            "work collaboratively with the marketing team\n",
            "amplify our reach\n",
            "boost our sales figures\n",
            "marketing team\n",
            "sales figures\n",
            "=============================================================\n",
            "I would like to schedule a meeting next week to discuss how we can further capitalize on our current success and explore new sales strategies.\n",
            "schedule a meeting\n",
            "capitalize on our current success\n",
            "explore new sales strategies\n",
            "next week\n",
            "current success\n",
            "new sales strategies\n",
            "=============================================================\n",
            "Please let me know your availability so we can set a time that works for you.\n",
            "know your availability\n",
            "set a time\n",
            "work for you\n",
            "time works for\n",
            "=============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence embeddings from Sentence Transformers ##"
      ],
      "metadata": {
        "id": "A_FPJ8NrMxVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = SentenceTransformer('all-distilroberta-v1')"
      ],
      "metadata": {
        "id": "PGX_xGJ2MNN3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verb_phrases_from_text_embeddings = model1.encode(verb_phrases_from_text, convert_to_tensor=True)\n",
        "noun_phrases_from_text_embeddings = model1.encode(noun_phrases_from_text, convert_to_tensor=True)\n",
        "verb_standard_phrases_embeddings = model1.encode(verb_standard_phrases, convert_to_tensor=True)\n",
        "noun_standard_phrases_embeddings = model1.encode(noun_standard_phrases, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "3sVHQTUbF_JN"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verb_cosine_scores = util.cos_sim(verb_phrases_from_text_embeddings, verb_standard_phrases_embeddings).numpy()\n",
        "noun_cosine_scores = util.cos_sim(noun_phrases_from_text_embeddings, noun_standard_phrases_embeddings).numpy()"
      ],
      "metadata": {
        "id": "ubxZJ8pEF_Mb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verb phrases and replacements\n",
        "for i, phrase in enumerate(verb_phrases_from_text):\n",
        "    j = np.argmax(verb_cosine_scores[i])\n",
        "    if verb_cosine_scores[i][j] > 0.5:\n",
        "        print(\"{:50s} {:30s} Score: {:.4f}\".format(phrase, verb_standard_phrases[j], verb_cosine_scores[i][j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGfMIgLHb0En",
        "outputId": "e5fc08b3-bed8-4dc7-b758-7d2e8f8f6940"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "show a significant increase                        sales increase                 Score: 0.5712\n",
            "drive sales                                        drive sales                    Score: 1.0000\n",
            "engage with our customers                          enhance customer experience    Score: 0.7009\n",
            "reach our targets                                  achieve targets                Score: 0.6980\n",
            "employ by the marketing team                       target customers               Score: 0.5131\n",
            "resonate well with our target audience             target demographics            Score: 0.5010\n",
            "impact our sales                                   increase sales                 Score: 0.7293\n",
            "ride this wave of success                          build success                  Score: 0.5183\n",
            "review our sales tactics                           target customers               Score: 0.5464\n",
            "align them with the marketing strategies in        manage strategies              Score: 0.5593\n",
            "maximize our impact in the market                  maximize impact                Score: 0.5992\n",
            "work collaboratively with the marketing team       develop campaigns              Score: 0.5703\n",
            "amplify our reach                                  increase reach                 Score: 0.6996\n",
            "boost our sales figures                            increase sales                 Score: 0.7173\n",
            "schedule a meeting                                 facilitate meetings            Score: 0.6363\n",
            "capitalize on our current success                  capitalize on opportunities    Score: 0.6334\n",
            "explore new sales strategies                       increase sales                 Score: 0.6988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# noun phrases and replacements\n",
        "for i, phrase in enumerate(noun_phrases_from_text):\n",
        "    j = np.argmax(noun_cosine_scores[i])\n",
        "    if noun_cosine_scores[i][j] > 0.5:\n",
        "        print(\"{:50s} {:50s} Score: {:.4f}\".format(phrase, noun_standard_phrases[j], noun_cosine_scores[i][j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3wJYqLbbOSh",
        "outputId": "361bb6d9-d92c-4df2-a554-cd1004a1f20a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "significant increase                               revenue growth                                     Score: 0.5189\n",
            "substantial contribution to this success           business success                                   Score: 0.5035\n",
            "efforts in driving sales                           sales objectives                                   Score: 0.6221\n",
            "ongoing marketing campaign                         marketing campaign                                 Score: 0.8422\n",
            "innovative approach and creative                   business innovation                                Score: 0.6068\n",
            "marketing team                                     marketing campaign                                 Score: 0.6900\n",
            "wave of success                                    business success                                   Score: 0.5701\n",
            "good time to review sales tactics                  sales strategy                                     Score: 0.5932\n",
            "sales tactics                                      sales techniques                                   Score: 0.8417\n",
            "marketing strategies to maximize impact in the market marketing strategy                                 Score: 0.8060\n",
            "impact in the market                               market penetration                                 Score: 0.6681\n",
            "marketing team                                     marketing campaign                                 Score: 0.6900\n",
            "sales figures                                      sales growth                                       Score: 0.5930\n",
            "current success                                    business success                                   Score: 0.6640\n",
            "new sales strategies                               sales strategy                                     Score: 0.7888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embeddings from RoBERTa ##"
      ],
      "metadata": {
        "id": "dcNch9FGMUJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    \"\"\"\n",
        "        Obtains word embeddings from the model and averages them to get a sentence embedding\n",
        "    \"\"\"\n",
        "    token_embeddings = model_output['last_hidden_state']\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask"
      ],
      "metadata": {
        "id": "h3OFOMxUMS1W"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "model2 = RobertaModel.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXH0ifvno-_a",
        "outputId": "11afa2c1-00f6-4e93-d904-57bc710d863a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model2.to(device)\n",
        "model2.eval()\n",
        "\n",
        "verb_phrases_from_text_emb = torch.Tensor().to(device)\n",
        "verb_standard_phrases_emb = torch.Tensor().to(device)\n",
        "noun_phrases_from_text_emb = torch.Tensor().to(device)\n",
        "noun_standard_phrases_emb = torch.Tensor().to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for vp in verb_phrases_from_text:\n",
        "        inputs = tokenizer(vp, return_tensors=\"pt\")\n",
        "        outputs = model2(**inputs)\n",
        "        verb_phrases_from_text_emb = torch.cat([verb_phrases_from_text_emb,\n",
        "                                                mean_pooling(outputs, inputs['attention_mask'])])\n",
        "    verb_phrases_from_text_emb = verb_phrases_from_text_emb.cpu().numpy()\n",
        "    for vp in verb_standard_phrases:\n",
        "        inputs = tokenizer(vp, return_tensors=\"pt\")\n",
        "        outputs = model2(**inputs)\n",
        "        verb_standard_phrases_emb = torch.cat([verb_standard_phrases_emb,\n",
        "                                               mean_pooling(outputs, inputs['attention_mask'])])\n",
        "    verb_standard_phrases_emb = verb_standard_phrases_emb.cpu().numpy()\n",
        "    for np in noun_phrases_from_text:\n",
        "        inputs = tokenizer(np, return_tensors=\"pt\")\n",
        "        outputs = model2(**inputs)\n",
        "        noun_phrases_from_text_emb = torch.cat([noun_phrases_from_text_emb,\n",
        "                                                mean_pooling(outputs, inputs['attention_mask'])])\n",
        "    noun_phrases_from_text_emb = noun_phrases_from_text_emb.cpu().numpy()\n",
        "    for np in noun_standard_phrases:\n",
        "        inputs = tokenizer(np, return_tensors=\"pt\")\n",
        "        outputs = model2(**inputs)\n",
        "        noun_standard_phrases_emb = torch.cat([noun_standard_phrases_emb,\n",
        "                                               mean_pooling(outputs, inputs['attention_mask'])])\n",
        "    noun_standard_phrases_emb = noun_standard_phrases_emb.cpu().numpy()"
      ],
      "metadata": {
        "id": "c6cKDrVsM-I5"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verb_cosine_scores = cosine_similarity(verb_phrases_from_text_emb, verb_standard_phrases_emb)\n",
        "noun_cosine_scores = cosine_similarity(noun_phrases_from_text_emb, noun_standard_phrases_emb)"
      ],
      "metadata": {
        "id": "oS2WsVReOJ52"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verb_cosine_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4HDPnS8TNRy",
        "outputId": "29002a2a-acba-4d2a-a635-c782c219bdf4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.94674265, 0.9535094 , 0.93963057, ..., 0.9481195 , 0.94369954,\n",
              "        0.9481355 ],\n",
              "       [0.946267  , 0.9693363 , 0.94950885, ..., 0.94981474, 0.94590247,\n",
              "        0.9585757 ],\n",
              "       [0.9203085 , 0.9344268 , 0.9246465 , ..., 0.9277223 , 0.92426074,\n",
              "        0.93582684],\n",
              "       ...,\n",
              "       [0.95567465, 0.9663379 , 0.9492316 , ..., 0.9536645 , 0.9518582 ,\n",
              "        0.9675814 ],\n",
              "       [0.95434606, 0.9592153 , 0.941956  , ..., 0.9518529 , 0.9450792 ,\n",
              "        0.9603782 ],\n",
              "       [0.96144027, 0.964925  , 0.9429299 , ..., 0.9551538 , 0.94812214,\n",
              "        0.9633505 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, phrase in enumerate(verb_phrases_from_text):\n",
        "    j = np.argmax(verb_cosine_scores[i])\n",
        "    if verb_cosine_scores[i][j] > 0.5:\n",
        "        print(\"{:50s} {:30s} Score: {:.4f}\".format(phrase, verb_standard_phrases[j], verb_cosine_scores[i][j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "Uxl27Lt7Um5D",
        "outputId": "a8099d64-7816-4c69-d534-9e87ffe74f3b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'argmax'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-549efd73ce90>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb_phrases_from_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb_cosine_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverb_cosine_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:50s} {:30s} Score: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb_standard_phrases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb_cosine_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'argmax'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, phrase in enumerate(noun_phrases_from_text):\n",
        "    j = np.argmax(noun_cosine_scores[i])\n",
        "    if noun_cosine_scores[i][j] > 0.5:\n",
        "        print(\"{:50s} {:50s} Score: {:.4f}\".format(phrase, noun_standard_phrases[j], noun_cosine_scores[i][j]))"
      ],
      "metadata": {
        "id": "H7pFxoTsMmS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}